{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb99a456",
   "metadata": {},
   "source": [
    "# Homework 2 \n",
    "\n",
    "## ECS 271 \n",
    "\n",
    "## Kay Royo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b9fbf",
   "metadata": {},
   "source": [
    "### 1.) \n",
    "\n",
    "Implement a single-layer fully connected neural network model to classify MNIST images. It inputs\n",
    "a raw image as a 1D vector of length 784=28x28 and outputs a vector of length 10 (each dimension\n",
    "corresponds to a class). You may want to add a bias term to the input, but that is optional for this\n",
    "assignment. The output is connected to the first layer simply by a set of linear weights. The second\n",
    "layer uses Softmax function as the non-linearity function. Softmax is a simple function that converts a\n",
    "n-dimensional input (z) to a n-dimensional output (o) where the output sums to one and each element\n",
    "is a value between 0 and 1. It is defined as\n",
    "\n",
    "$$y_i = \\frac{exp(x_i)}{\\sum_{j=1}^n exp(x_j)}$$\n",
    "\n",
    "When we apply this function to the output of the network, $o$, it predicts a vector which can be seen as\n",
    "the probability of each category given the input $x$:\n",
    "\n",
    "$$P(c_i|x) = \\frac{exp(o_i)}{\\sum^n_{j=1} exp(o_j )}$$\n",
    "\n",
    "\n",
    "where $n$ is the number of categories, 10, in our case. We want the $i$’th output to mimic $P(c_i|x)$,\n",
    "the probability of the input $x$ belonging to the category $i$. We can represent the desired probability\n",
    "distribution as the vector $gt$ where $gt(i)$ is one only if the input is from the $i$’th category and zero\n",
    "otherwise. This is called one-hot encoding. Assuming $x$ is from $y$’th category, $gt(y)$ is the only element\n",
    "in $gt$ that is equal to one. Then, we want the output probability distribution to be similar to the desired\n",
    "one (ground-truth). Hence, we use cross-entropy loss to compare these two probability distributions,\n",
    "$P$ and $gt$:\n",
    "\n",
    "$$L(x,y,w) = \\sum_{i=1}^n - gt(i) log(P(c_i|x))$$\n",
    "\n",
    "where $n$ is the number of categories. Since $gt$ is one hot encoding, we can remove the terms of $gt$ that\n",
    "are zero, keeping only the $y$’th term. Since $gt(y) = 1$, we can remove it in the multiplication to come\n",
    "up with the following loss which is identical to the above one:\n",
    "\n",
    "$$L(x,y,w) = - log(P(c_y|x))$$\n",
    "\n",
    "This is the loss for one input only, so the total loss on a mini-batch is:\n",
    "\n",
    "$$L = \\sum_{k=1}^N -log(P(c_{yk}|x_k)) $$\n",
    "\n",
    "where $N$ is the size of mini-batch, number of training data being processed at this iteration.\n",
    "\n",
    "Please implement stochastic gradient descend (SGD) algorithm from scratch to train the model. You\n",
    "may use NumPy, but should not use PyTorch, TensorFlow, or any similar deep learning framework.\n",
    "Use mini-batch of 10 images per iteration. Then, train it on all MNIST train dataset and plot the\n",
    "accuracy on all test data for every n iteration. Please choose n small enough so that the graph shows\n",
    "the progress of learning and large enough so that testing does not take a lot of time. You may use\n",
    "smaller n initially and then increase it gradually as the learning progresses. Choose a learning rate so\n",
    "that the loss goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd4c25",
   "metadata": {},
   "source": [
    "*Answer:*\n",
    "\n",
    "(TODO: easiest way to get the gradient of loss w.r.t. weights will be to derive the derivative manually on paper to get the final formula (probably using chain rule manually) and then write a function that calculates the formula in Python. Please include you derivation for the formula in you report by typing it in Latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae218fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies \n",
    "import pandas as pd \n",
    "import nbconvert\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import operator \n",
    "from operator import itemgetter\n",
    "import plotly.express as px\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ebecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 1)\n",
      "(10000, 784) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load dataset \n",
    "M = loadmat('..\\HW1\\data\\MNIST_digit_data.mat')\n",
    "images_train,images_test,labels_train,labels_test= M['images_train'],M['images_test'],M['labels_train'],M['labels_test']\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#randomly permute data points\n",
    "inds = np.random.permutation(images_train.shape[0])\n",
    "images_train = images_train[inds]\n",
    "labels_train = labels_train[inds]\n",
    "\n",
    "inds = np.random.permutation(images_test.shape[0])\n",
    "images_test = images_test[inds]\n",
    "labels_test = labels_test[inds]\n",
    "\n",
    "print(images_train.shape, labels_train.shape)\n",
    "print(images_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffb877ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6e3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding function \n",
    "def one_hot_enc(Y):\n",
    "    t = np.zeros((Y.shape[0], 10))\n",
    "    for i in range(Y.shape[0]):\n",
    "        t[i][int(Y[i][0])] = 1 \n",
    "    return t\n",
    "\n",
    "#normalization function \n",
    "def normalize(X): \n",
    "    X = X / 255\n",
    "    return X \n",
    "\n",
    "images_train_norm = normalize(images_train)\n",
    "labels_train = one_hot_enc(labels_train)\n",
    "images_test_norm = normalize(images_test)\n",
    "labels_test = one_hot_enc(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd69b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b527c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradient descent algorithm\n",
    "class neural_net: \n",
    "    #initialize empty layer dicts \n",
    "    l1 = {} #matrix of size (784,10)\n",
    "    l2 = {} #matrix of size(10,10)\n",
    "        \n",
    "    #initialize model parameters, such as 1st & 2nd layer parameters and biases\n",
    "    def __init__(self, inputs, hidden, outputs):\n",
    "        self.input_size = inputs\n",
    "        self.hidden_size = hidden\n",
    "        self.output_size = outputs\n",
    "        self.l1['params'] = np.random.randn(inputs, hidden) / np.sqrt(hidden) #layer1 (normalized)\n",
    "        self.l1['bias'] = np.random.randn(hidden,1) / np.sqrt(hidden) #include bias \n",
    "        self.l2['params'] = np.random.randn(hidden,outputs) / np.sqrt(hidden) #layer2 (normalized)\n",
    "        self.l2['bias'] = np.random.randn(outputs,1) / np.sqrt(hidden) #include bias\n",
    "    \n",
    "    #activation functions \n",
    "    def softmax(self, x):\n",
    "    #simplified to prevent overflow\n",
    "        exp = np.exp(x - x.max())\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "    \n",
    "    def dsoftmax(self, X):\n",
    "        #derivative of softmax\n",
    "        exp=np.exp(X-X.max())\n",
    "        return exp/np.sum(exp,axis=0)*(1-exp/np.sum(exp,axis=0))\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(0,X)\n",
    "\n",
    "    def drelu(self, X):\n",
    "        #derivative of relu \n",
    "        return 1 * (X > 0) \n",
    "    \n",
    "    #cross entropy error\n",
    "    def cross_entropy_err(self, y,y_p):\n",
    "        loss = -np.sum(y*np.log(y_p))\n",
    "        return loss/float(y_p.shape[0])\n",
    "    \n",
    "    #forward pass \n",
    "    def forward(self,X,Y):\n",
    "        # input layer to l1 \n",
    "        X_l1 = np.matmul((self.l1['params']),X) + self.l1['bias']\n",
    "        X_relu = np.array(self.relu(X_l1))\n",
    "        # l1 to output layer\n",
    "        X_l2 = np.matmul(self.l2['params'],X_relu).reshape((self.output_size,1)) + self.l2['bias']\n",
    "        out_put = np.squeeze(self.softmax(X_l2)) #compute predicted list using softmax \n",
    "        #compute error \n",
    "        error = self.cross_entropy_err(Y,out_put) \n",
    "        #save results in dictionary \n",
    "        dict_f = {'X_l1':X_l1, 'X_relu':X_relu, 'X_l2':X_l2, 'Y_pred':out_put.reshape((1,self.output_size)), 'error':error }\n",
    "        \n",
    "        return dict_f \n",
    "    \n",
    "    #back propagation   \n",
    "    def backward(self,X,Y,F):\n",
    "        #one-hot encoded labels\n",
    "        targets = np.array([0]*self.output_size).reshape((1,self.output_size))\n",
    "        targets[0][Y] = 1\n",
    "        \n",
    "        #compute l2 update \n",
    "        err2 = 2*(F['Y_pred']-targets)/F['Y_pred'].shape[0]*self.dsoftmax(F['X_l2'])\n",
    "        l2_update = np.matmul(err2,F['X_relu'])\n",
    "        \n",
    "        #compute l1 update\n",
    "        err1 = (np.matmul(self.l2['params'].transpose(),err2)).transpose()*self.drelu(F['X_l1'])\n",
    "        l1_update = err1*self.relu(F['X_l1'])\n",
    "\n",
    "        dict_b = {\n",
    "            'err2':err2,\n",
    "            'l2_update':l2_update,\n",
    "            'err1':err1,\n",
    "            'l1_update':l1_update\n",
    "        }\n",
    "        return dict_b\n",
    "    \n",
    "    #loss function of training set\n",
    "    def loss_func(self,X_tr,Y_tr):\n",
    "        loss = 0\n",
    "        for n in range(len(X_train)):\n",
    "            y = Y_tr[n]\n",
    "            x = X_tr[n][:]\n",
    "            loss += self.forward(x,y)['error']\n",
    "        return loss\n",
    "    \n",
    "    #training\n",
    "    def train(self, X_tr, Y_tr, n = 1000, batch = 10, learning_rate = 0.01):\n",
    "        \n",
    "        test_dict = {} \n",
    "        loss_dict = {}\n",
    "        count = 1\n",
    "        for i in range(n): \n",
    "            # generate a random indices for the training set\n",
    "            indices = np.random.choice(len(X_tr), batch, replace=True)\n",
    "            F = self.forward(X_tr[indices],Y_tr[indices])\n",
    "            B = self.backward(X_tr[indices],Y_tr[indices],F)\n",
    "            #update hyperparams\n",
    "            self.l1['params'] -= learning_rate*B['l1_update']\n",
    "            self.l1['bias'] -= learning_rate*B['err1']\n",
    "            self.l2['params'] -= learning_rate*B['l2_update']\n",
    "            self.l2['bias'] -= learning_rate*B['err2']\n",
    "\n",
    "            if count % 1000 == 0:\n",
    "                    loss = self.loss_func(X_tr,Y_tr)\n",
    "                    test = self.test(x_test,y_test)\n",
    "                    print('Trained for {} times,'.format(count),'loss = {}, test = {}'.format(loss,test))\n",
    "                    loss_dict[str(count)]=loss\n",
    "                    test_dict[str(count)]=test\n",
    "            count += 1\n",
    "        print('Training completed.')\n",
    "        return loss_dict, test_dict\n",
    "    \n",
    "    #testing \n",
    "    def test(self,X_te, Y_te):\n",
    "        correct = 0\n",
    "        for n in range(len(X_te)):\n",
    "            x = X_te[n][:]\n",
    "            y = Y_te[n]\n",
    "            pred = np.argmax(self.forward(x,y)['Y_pred'])\n",
    "            for n in range(len(y))\n",
    "            if (prediction == y):\n",
    "                correct += 1 #sum correctly identified labels \n",
    "        print('Test Accuarcy: ',correct/len(X_te))\n",
    "        return correct/np.float(len(X_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ab3c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 20000 #number of iterations\n",
    "lr = 0.01 #learning rate \n",
    "batch = 10 #batch size \n",
    "input_size = 28*28 #num inputs\n",
    "output_size = 10 #num outputs\n",
    "hidden_size = 10 #hidden layer size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bbcb4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_net(input_size,hidden_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "98360f01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (784,784) (10,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_dict, test_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36mneural_net.train\u001b[1;34m(self, X_tr, Y_tr, n, batch, learning_rate)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n): \n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# generate a random indices for the training set\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(X_tr), batch, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m     F \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(X_tr[indices],Y_tr[indices],F)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m#update hyperparams\u001b[39;00m\n",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36mneural_net.forward\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,Y):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# input layer to l1 \u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     X_l1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbias\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     44\u001b[0m     X_relu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(X_l1))\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# l1 to output layer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,784) (10,1) "
     ]
    }
   ],
   "source": [
    "loss_dict, test_dict = model.train(images_train,labels_train,n=n,batch=batch, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ede1210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = {}\n",
    "l1['params'] = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "l1['params'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "409b6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(x,y):\n",
    "    layer=np.random.uniform(-1.,1.,size=(x,y))/np.sqrt(x*y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "np.random.seed(42)\n",
    "l1=init(28*28,128)\n",
    "l2=init(128,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3f0fdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.9208513e-04,  2.8455637e-03,  1.4646830e-03, ...,\n",
       "        -4.6020158e-04,  2.0077715e-03,  2.2774558e-03],\n",
       "       [-3.1128346e-03,  6.7852598e-05, -5.2142178e-04, ...,\n",
       "        -1.1558544e-03, -2.0866422e-03,  3.5861213e-04],\n",
       "       [ 2.7536429e-03,  1.2376250e-03,  4.4232793e-04, ...,\n",
       "         1.2420909e-03,  8.1407465e-04,  2.3831520e-03],\n",
       "       ...,\n",
       "       [-4.8830349e-05,  6.0965895e-04, -2.5888244e-03, ...,\n",
       "        -2.5404603e-03,  3.0673348e-04,  5.6949357e-04],\n",
       "       [ 4.0616604e-04,  2.9352377e-03, -1.1387233e-03, ...,\n",
       "         2.8009422e-03, -2.8184371e-03,  1.9042534e-04],\n",
       "       [-3.7994463e-04,  1.5051493e-03, -1.2513590e-03, ...,\n",
       "         1.4320496e-03,  1.6163822e-03, -1.6940964e-03]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e0cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic (mini-batch) gradient descent algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy on all test data for every n iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d2129",
   "metadata": {},
   "source": [
    "### 2.) \n",
    "\n",
    "For each class, visualize the 10 images that are misclassified with the highest score along with their\n",
    "predicted label and score. These are very confident wrong predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c265d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ec69cb6",
   "metadata": {},
   "source": [
    "### 3.) \n",
    "\n",
    "Please reduce the number of training data to 1 example per class (chosen randomly from training data)\n",
    "and plot the curve (accuracy vs, iterations). The whole training data will be 10 images only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747085a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5a5e5a3",
   "metadata": {},
   "source": [
    "### 4.) \n",
    "\n",
    "Try different mini-batch sizes (1, 10, 100) for the original case and plot the results. Which one is better\n",
    "and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee690e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6eed255",
   "metadata": {},
   "source": [
    "### 5.) \n",
    "\n",
    "Instead of using random sampling, sort the data before training so that all ”1”s appear before ”2”s\n",
    "and so on. Then, sample sequentially in running SGD instead of random sampling. Does this work\n",
    "well, why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021ad59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51254ffd",
   "metadata": {},
   "source": [
    "### 6.) \n",
    "\n",
    "(Bonus point) Add a hidden layer with 11 hidden neurons and ReLU activation function. Then, plot\n",
    "the accuracy curve to see how the accuracy changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9d1b3",
   "metadata": {},
   "source": [
    "Note: if you want to solve the bonus question since there are two more layers (a linear layer and a ReLU activation layer). If you want to do the bonus too, you may want to code back-propagation by calculating the derivative formula for each layer and hard-coding it into your python code. Then, you can write code that multiplies them to get the final gradient as discussed in the class. If you go this route, you can use it for both bonus and non-bonus parts of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4867e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
