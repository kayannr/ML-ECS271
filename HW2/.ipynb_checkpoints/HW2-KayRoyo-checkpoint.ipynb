{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb99a456",
   "metadata": {},
   "source": [
    "# Homework 2 \n",
    "\n",
    "## ECS 271 \n",
    "\n",
    "## Kay Royo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b9fbf",
   "metadata": {},
   "source": [
    "### 1.) \n",
    "\n",
    "Implement a single-layer fully connected neural network model to classify MNIST images. It inputs\n",
    "a raw image as a 1D vector of length 784=28x28 and outputs a vector of length 10 (each dimension\n",
    "corresponds to a class). You may want to add a bias term to the input, but that is optional for this\n",
    "assignment. The output is connected to the first layer simply by a set of linear weights. The second\n",
    "layer uses Softmax function as the non-linearity function. Softmax is a simple function that converts a\n",
    "n-dimensional input (z) to a n-dimensional output (o) where the output sums to one and each element\n",
    "is a value between 0 and 1. It is defined as\n",
    "\n",
    "$$y_i = \\frac{exp(x_i)}{\\sum_{j=1}^n exp(x_j)}$$\n",
    "\n",
    "When we apply this function to the output of the network, $o$, it predicts a vector which can be seen as\n",
    "the probability of each category given the input $x$:\n",
    "\n",
    "$$P(c_i|x) = \\frac{exp(o_i)}{\\sum^n_{j=1} exp(o_j )}$$\n",
    "\n",
    "\n",
    "where $n$ is the number of categories, 10, in our case. We want the $i$’th output to mimic $P(c_i|x)$,\n",
    "the probability of the input $x$ belonging to the category $i$. We can represent the desired probability\n",
    "distribution as the vector $gt$ where $gt(i)$ is one only if the input is from the $i$’th category and zero\n",
    "otherwise. This is called one-hot encoding. Assuming $x$ is from $y$’th category, $gt(y)$ is the only element\n",
    "in $gt$ that is equal to one. Then, we want the output probability distribution to be similar to the desired\n",
    "one (ground-truth). Hence, we use cross-entropy loss to compare these two probability distributions,\n",
    "$P$ and $gt$:\n",
    "\n",
    "$$L(x,y,w) = \\sum_{i=1}^n - gt(i) log(P(c_i|x))$$\n",
    "\n",
    "where $n$ is the number of categories. Since $gt$ is one hot encoding, we can remove the terms of $gt$ that\n",
    "are zero, keeping only the $y$’th term. Since $gt(y) = 1$, we can remove it in the multiplication to come\n",
    "up with the following loss which is identical to the above one:\n",
    "\n",
    "$$L(x,y,w) = - log(P(c_y|x))$$\n",
    "\n",
    "This is the loss for one input only, so the total loss on a mini-batch is:\n",
    "\n",
    "$$L = \\sum_{k=1}^N -log(P(c_{yk}|x_k)) $$\n",
    "\n",
    "where $N$ is the size of mini-batch, number of training data being processed at this iteration.\n",
    "\n",
    "Please implement stochastic gradient descend (SGD) algorithm from scratch to train the model. You\n",
    "may use NumPy, but should not use PyTorch, TensorFlow, or any similar deep learning framework.\n",
    "Use mini-batch of 10 images per iteration. Then, train it on all MNIST train dataset and plot the\n",
    "accuracy on all test data for every n iteration. Please choose n small enough so that the graph shows\n",
    "the progress of learning and large enough so that testing does not take a lot of time. You may use\n",
    "smaller n initially and then increase it gradually as the learning progresses. Choose a learning rate so\n",
    "that the loss goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd4c25",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe73412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a45709",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_data = h5py.File('MNISTdata.hdf5', 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:])\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0]))\n",
    "x_test = np.float32(MNIST_data['x_test'][:])\n",
    "y_test = np.int32(np.array(MNIST_data['y_test'][:,0]))\n",
    "MNIST_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c67f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a53b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae218fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies \n",
    "import pandas as pd \n",
    "import nbconvert\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import operator \n",
    "from operator import itemgetter\n",
    "import plotly.express as px\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ebecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 1)\n",
      "(10000, 784) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load dataset \n",
    "M = loadmat('..\\HW1\\data\\MNIST_digit_data.mat')\n",
    "images_train,images_test,labels_train,labels_test= M['images_train'],M['images_test'],M['labels_train'],M['labels_test']\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#randomly permute data points\n",
    "inds = np.random.permutation(images_train.shape[0])\n",
    "images_train = images_train[inds]\n",
    "labels_train = labels_train[inds]\n",
    "\n",
    "inds = np.random.permutation(images_test.shape[0])\n",
    "images_test = images_test[inds]\n",
    "labels_test = labels_test[inds]\n",
    "\n",
    "print(images_train.shape, labels_train.shape)\n",
    "print(images_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb877ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6e3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding function \n",
    "def one_hot_enc(Y):\n",
    "    t = np.zeros((Y.shape[0], 10))\n",
    "    for i in range(Y.shape[0]):\n",
    "        t[i][int(Y[i][0])] = 1 \n",
    "    return t\n",
    "\n",
    "#normalization function \n",
    "def normalize(X): \n",
    "    X = X / 255\n",
    "    return X \n",
    "\n",
    "images_train_norm = normalize(images_train)\n",
    "labels_train = one_hot_enc(labels_train)\n",
    "images_test_norm = normalize(images_test)\n",
    "labels_test = one_hot_enc(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd69b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b527c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradient descent algorithm\n",
    "class neural_net: \n",
    "\n",
    "    #initialize model parameters, such as 1st & 2nd layer parameters and biases\n",
    "    def __init__(self, inputs, hidden, outputs):\n",
    "        #initialize empty layer dicts \n",
    "        l1 = {} #matrix of size (784,10)\n",
    "        l2 = {} #matrix of size(10,10)\n",
    "        self.input_size = inputs\n",
    "        self.hidden_size = hidden\n",
    "        self.output_size = outputs\n",
    "        self.l1['params'] = np.random.randn(inputs, hidden) / np.sqrt(hidden) #layer1\n",
    "        self.l1['bias'] = np.random.randn(hidden,1) / np.sqrt(hidden) #include bias \n",
    "        self.l2['params'] = np.random.randn(hidden,outputs) / np.sqrt(hidden) #layer2 \n",
    "        self.l2['bias'] = np.random.randn(outputs,1) / np.sqrt(hidden) #include bias\n",
    "    \n",
    "    #activation functions \n",
    "    def softmax(self, x):\n",
    "    #simplified to prevent overflow\n",
    "        exp = np.exp(x - x.max())\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "    \n",
    "    def dsoftmax(X):\n",
    "        #derivative of softmax\n",
    "        exp=np.exp(X-X.max())\n",
    "        return exp/np.sum(exp,axis=0)*(1-exp/np.sum(exp,axis=0))\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(0,X)\n",
    "\n",
    "    def drelu(self, X):\n",
    "        #derivative of relu \n",
    "        return 1 * (X > 0) \n",
    "    \n",
    "    #cross entropy error\n",
    "    def cross_entropy_err(y,y_p):\n",
    "        loss = -np.sum(y*np.log(y_p))\n",
    "        return loss/float(y_p.shape[0])\n",
    "    \n",
    "    #forward pass \n",
    "    def forward(self,X,Y):\n",
    "        # input layer to l1 \n",
    "        X_l1 = np.matmul(self.l1['params'],X).reshape((self.hidden_size,1)) + self.l1['bias']\n",
    "        X_relu = np.array(self.relu(X_l1)).reshape((self.hidden_size,1))\n",
    "        # l1 to output layer\n",
    "        X_l2 = np.matmul(self.l2['params'],X_relu).reshape((self.output_size,1)) + self.l2['bias']\n",
    "        out_put = np.squeeze(self.softmax(X_l2)) #compute predicted list using softmax \n",
    "        #compute error \n",
    "        error = self.cross_entropy_err(Y,out_put) \n",
    "        #save results in dictionary \n",
    "        dict_f = {'X_l1':X_l1, 'X_relu':X_relu, 'X_l2':X_l2, 'Y_pred':out_put.reshape((1,self.output_size)), 'error':error }\n",
    "        \n",
    "        return dict_f \n",
    "    \n",
    "    #back propagation   \n",
    "    def backward(self,X,Y,F):\n",
    "        #one-hot encode labels\n",
    "        targets = np.array([0]*self.output_size).reshape((1,self.output_size))\n",
    "        targets[0][y] = 1\n",
    "        \n",
    "        #compute l2 update \n",
    "        err2 = 2*(F['Y_pred']-targets)/F['Y_pred'].shape[0]*self.dsoftmax(X_l2)\n",
    "        l2_update = np.matmul(err2,F['X_relu'].transpose())\n",
    "        \n",
    "        #compute l1 update\n",
    "        err1 = (np.matmul(self.l2['params'].transpose(),err2)).transpose()*self.drelu(x_l1)\n",
    "        l1_update = err1.reshape(self.hidden_size,1)*self.relu(F['X_l1']).reshape(self.hidden_size,1)\n",
    "\n",
    "        dict_b = {\n",
    "            'err2':err2,\n",
    "            'l2_update':l2_update,\n",
    "            'err1':err1,\n",
    "            'l1_update':l1_update\n",
    "        }\n",
    "        return dict_b\n",
    "    \n",
    "    #loss function of training set\n",
    "    def loss_func(self,X_tr,Y_tr):\n",
    "        loss = 0\n",
    "        for n in range(len(X_train)):\n",
    "            y = Y_tr[n]\n",
    "            x = X_tr[n][:]\n",
    "            loss += self.forward(x,y)['error']\n",
    "        return loss\n",
    "    \n",
    "    #update hyperparams \n",
    "    def update(self,B, lr):\n",
    "        self.l1['params'] -= lr*B['l1_update']\n",
    "        self.l1['bias'] -= lr*B['err1']\n",
    "        self.l2['params'] -= lr*B['l2_update']\n",
    "        self.l2['bias'] -= lr*B['err2']\n",
    "    \n",
    "    #training\n",
    "    def train(self, X_tr, Y_tr, n = 1000, learning_rate = 0.01):\n",
    "        # generate a random indices for the training set\n",
    "        indices = np.random.choice(len(X_tr), n, replace=True)\n",
    "        count = 1\n",
    "        test_dict = {} \n",
    "        loss_dict = {}\n",
    "        for i in rand_indices:\n",
    "            F = self.forward(X_tr[i],Y_tr[i])\n",
    "            B = self.backward(X_tr[i],Y_tr[i],F)\n",
    "            self.update(B,learning_rate)\n",
    "            \n",
    "            if count % 1000 == 0:\n",
    "                if count % 5000 == 0:\n",
    "                    loss = self.loss_func(X_tr,Y_tr)\n",
    "                    test = self.test(x_test,y_test)\n",
    "                    print('Trained for {} times,'.format(count),'loss = {}, test = {}'.format(loss,test))\n",
    "                    loss_dict[str(count)]=loss\n",
    "                    test_dict[str(count)]=test\n",
    "                else:\n",
    "                    print('Trained for {} times,'.format(count))\n",
    "            count += 1\n",
    "\n",
    "        print('Training complete')\n",
    "        return loss_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ab3c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([0]*10).reshape((1,10))\n",
    "\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb4c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98360f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ede1210e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "409b6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(x,y):\n",
    "    layer=np.random.uniform(-1.,1.,size=(x,y))/np.sqrt(x*y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "np.random.seed(42)\n",
    "l1=init(28*28,128)\n",
    "l2=init(128,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3f0fdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.9208513e-04,  2.8455637e-03,  1.4646830e-03, ...,\n",
       "        -4.6020158e-04,  2.0077715e-03,  2.2774558e-03],\n",
       "       [-3.1128346e-03,  6.7852598e-05, -5.2142178e-04, ...,\n",
       "        -1.1558544e-03, -2.0866422e-03,  3.5861213e-04],\n",
       "       [ 2.7536429e-03,  1.2376250e-03,  4.4232793e-04, ...,\n",
       "         1.2420909e-03,  8.1407465e-04,  2.3831520e-03],\n",
       "       ...,\n",
       "       [-4.8830349e-05,  6.0965895e-04, -2.5888244e-03, ...,\n",
       "        -2.5404603e-03,  3.0673348e-04,  5.6949357e-04],\n",
       "       [ 4.0616604e-04,  2.9352377e-03, -1.1387233e-03, ...,\n",
       "         2.8009422e-03, -2.8184371e-03,  1.9042534e-04],\n",
       "       [-3.7994463e-04,  1.5051493e-03, -1.2513590e-03, ...,\n",
       "         1.4320496e-03,  1.6163822e-03, -1.6940964e-03]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e0cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200000 #num of iterations\n",
    "learning_rate = 0.01 #base learning rate\n",
    "num_inputs = 28*28 #num of inputs\n",
    "num_outputs = 10 #num of outputs\n",
    "hidden_size = 10 #batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy on all test data for every n iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d2129",
   "metadata": {},
   "source": [
    "### 2.) \n",
    "\n",
    "For each class, visualize the 10 images that are misclassified with the highest score along with their\n",
    "predicted label and score. These are very confident wrong predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c265d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ec69cb6",
   "metadata": {},
   "source": [
    "### 3.) \n",
    "\n",
    "Please reduce the number of training data to 1 example per class (chosen randomly from training data)\n",
    "and plot the curve (accuracy vs, iterations). The whole training data will be 10 images only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747085a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5a5e5a3",
   "metadata": {},
   "source": [
    "### 4.) \n",
    "\n",
    "Try different mini-batch sizes (1, 10, 100) for the original case and plot the results. Which one is better\n",
    "and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee690e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6eed255",
   "metadata": {},
   "source": [
    "### 5.) \n",
    "\n",
    "Instead of using random sampling, sort the data before training so that all ”1”s appear before ”2”s\n",
    "and so on. Then, sample sequentially in running SGD instead of random sampling. Does this work\n",
    "well, why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021ad59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51254ffd",
   "metadata": {},
   "source": [
    "### 6.) \n",
    "\n",
    "(Bonus point) Add a hidden layer with 11 hidden neurons and ReLU activation function. Then, plot\n",
    "the accuracy curve to see how the accuracy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4867e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
